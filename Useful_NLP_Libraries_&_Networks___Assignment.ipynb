{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Useful NLP Libraries & Networks | Assignment"
      ],
      "metadata": {
        "id": "SRlpK5c-LOpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use, and performance**.\n",
        "\n",
        "NLTK is mainly an educational and research-focused library that offers a wide range of NLP tools like tokenization, stemming, parsing, and corpora access, but it often requires more code and manual setup, making it less beginner-friendly for production use. spaCy, on the other hand, is designed for industrial and real-world applications, providing fast, pre-trained models for tasks such as POS tagging, NER, and dependency parsing with a very clean and easy-to-use API. In terms of performance, spaCy is significantly faster and more memory-efficient, while NLTK is more flexible for learning concepts and experimentation but slower and less optimized for large-scale applications."
      ],
      "metadata": {
        "id": "4atHnByELSf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is TextBlob and how does it simplify common** **NLP tasks like** **sentiment analysis and translation?**\n",
        "\n",
        "TextBlob is a high-level Python NLP library built on top of NLTK and Pattern that provides a very simple and intuitive API for common text processing tasks. It simplifies NLP by offering ready-to-use methods for tasks like sentiment analysis, part-of-speech tagging, noun phrase extraction, spelling correction, and translation, often in just one or two lines of code. For example, sentiment analysis can be done using a built-in polarity and subjectivity score, and translation is handled through simple method calls without requiring deep knowledge of underlying NLP models, making TextBlob especially useful for beginners and quick prototyping."
      ],
      "metadata": {
        "id": "bcRaab2KNUlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Explain the role of Standford NLP in academic and industry NLP Projects**.\n",
        "\n",
        "Stanford NLP plays an important role in both academic research and industry NLP projects by providing state-of-the-art, research-backed tools for core NLP tasks such as tokenization, POS tagging, named entity recognition, parsing, coreference resolution, and sentiment analysis. In academia, it is widely used for research, experimentation, and benchmarking due to its strong theoretical foundations and high-quality models. In industry, Stanford NLP (especially Stanford CoreNLP) is valued for its accuracy, language support, and robustness, making it suitable for building reliable NLP pipelines in applications like information extraction, question answering, and text analytics, particularly in systems where correctness and linguistic depth are more important than speed.\n"
      ],
      "metadata": {
        "id": "IC3-q2A4NmvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Describe the architecture and functioning of a Recurrent Natural Network (RNN)**\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a neural network architecture designed to handle sequential data such as text, speech, or time-series data. Its key feature is the presence of recurrent connections, which allow information from previous time steps to be passed forward as a hidden state, enabling the network to capture temporal dependencies. At each time step, the RNN takes the current input and the previous hidden state to produce a new hidden state and output using shared weights across the sequence. This architecture allows RNNs to model sequence order and context, but standard RNNs suffer from problems like vanishing and exploding gradients, which limit their ability to learn long-term dependencies—an issue addressed by improved variants such as LSTM and GRU."
      ],
      "metadata": {
        "id": "nadrjAfWN685"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is the key difference between LSTM and GRU networks in NLP**\n",
        "**applications?**\n",
        "\n",
        "The key difference between LSTM and GRU networks lies in their internal gating mechanisms. LSTM uses three gates (input, forget, and output) along with a separate cell state to control the flow of information, making it powerful for capturing long-term dependencies but computationally heavier. GRU, on the other hand, combines these functions into two gates (reset and update) and does not use a separate cell state, resulting in a simpler architecture with fewer parameters. In NLP applications, GRUs are generally faster to train and more memory-efficient, while LSTMs may perform slightly better on tasks requiring very long-range context."
      ],
      "metadata": {
        "id": "H3DUAtuUOB85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program using TextBlob to perform sentiment analysis on\n",
        "# the following paragraph of text:\n",
        "# “I had a great experience using the new mobile banking app. The interface is intuitive,\n",
        "# and customer support was quick to resolve my issue. However, the app did crash once\n",
        "# during a transaction, which was frustrating\"\n",
        "# Your program should print out the polarity and subjectivity scores.\n",
        "\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n",
        "\n",
        "from textblob import TextBlob\n",
        "text = \"\"\"I had a great experience using the new mobile banking app. The interface is intuitive,\n",
        "and customer support was quick to resolve my issue. However, the app did crash once\n",
        "during a transaction, which was frustrating\"\"\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "print(blob.sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkaCMYgDLRfz",
        "outputId": "f0493753-d21a-4af9-889a-3796e320d451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "Sentiment(polarity=0.21742424242424244, subjectivity=0.6511363636363636)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "# frequency distribution using Python and NLTK:\n",
        "# “Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "# computer science, and artificial intelligence. It enables machines to understand,\n",
        "# interpret, and generate human language. Applications of NLP include chatbots,\n",
        "# sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "# in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "\n",
        "from  nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\\n\", tokens)\n",
        "freq_dist = FreqDist(tokens)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(word, \":\", freq)"
      ],
      "metadata": {
        "id": "S3FEY4xzRo9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bb40a7-7db9-430a-b434-32b2167e8bb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "Natural : 1\n",
            "Language : 1\n",
            "Processing : 1\n",
            "( : 1\n",
            "NLP : 3\n",
            ") : 1\n",
            "is : 2\n",
            "a : 1\n",
            "fascinating : 1\n",
            "field : 1\n",
            "that : 1\n",
            "combines : 1\n",
            "linguistics : 1\n",
            ", : 7\n",
            "computer : 1\n",
            "science : 1\n",
            "and : 3\n",
            "artificial : 1\n",
            "intelligence : 1\n",
            ". : 4\n",
            "It : 1\n",
            "enables : 1\n",
            "machines : 1\n",
            "to : 1\n",
            "understand : 1\n",
            "interpret : 1\n",
            "generate : 1\n",
            "human : 1\n",
            "language : 1\n",
            "Applications : 1\n",
            "of : 2\n",
            "include : 1\n",
            "chatbots : 1\n",
            "sentiment : 1\n",
            "analysis : 1\n",
            "machine : 1\n",
            "translation : 1\n",
            "As : 1\n",
            "technology : 1\n",
            "advances : 1\n",
            "the : 1\n",
            "role : 1\n",
            "in : 1\n",
            "modern : 1\n",
            "solutions : 1\n",
            "becoming : 1\n",
            "increasingly : 1\n",
            "critical : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Implement a basic LSTM model in Keras for a text classification task using\n",
        "# the following dummy dataset. Your model should classify sentences as either positive\n",
        "# (1) or negative (0).\n",
        "# # Dataset\n",
        "# texts = [\n",
        "# “I love this project”, #Positive\n",
        "# “This is an amazing experience”, #Positive\n",
        "# “I hate waiting in line”, #Negative\n",
        "# “This is the worst service”, #Negative\n",
        "# “Absolutely fantastic!” #Positive\n",
        "# ]\n",
        "# labels = [1, 1, 0, 0, 1]\n",
        "# Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
        "# this data. You may use Keras with TensorFlow backend.\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "texts = [\n",
        "\"I love this project\", #Positive\n",
        "\"This is an amazing experience\", #Positive\n",
        "\"I hate waiting in line\", #Negative\n",
        "\"This is the worst service\", #Negative\n",
        "\"Absolutely fantastic!\" #Positive\n",
        "]\n",
        "\n",
        "labels = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "max_len = 6\n",
        "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_len),\n",
        "    LSTM(16),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X, labels, epochs=20, verbose=1)\n",
        "\n",
        "test = [\"I love this service\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "prediction = model.predict(test_pad)\n",
        "\n",
        "print(\"\\nPrediction:\", prediction)\n",
        "print(\"Class:\", 1 if prediction > 0.5 else 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w73CVJSWMQGh",
        "outputId": "bade6e7d-b4ae-4da2-fd13-bcc6de3fb2e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6000 - loss: 0.6892\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6873\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6000 - loss: 0.6854\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6835\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6815\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6795\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6000 - loss: 0.6774\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6753\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 0.6731\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6000 - loss: 0.6708\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6684\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6000 - loss: 0.6660\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 0.6634\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6607\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6579\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6550\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6520\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6000 - loss: 0.6488\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6000 - loss: 0.6455\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6420\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
            "\n",
            "Prediction: [[0.5565937]]\n",
            "Class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
        "# lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "# “Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "# development of India’s atomic energy program. He was the founding director of the Tata\n",
        "# # Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "# Atomic Energy Commission of India.”\n",
        "# Write a Python program that processes this text using spaCy, then prints tokens, their\n",
        "# lemmas, and any named entities found.\n",
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.\"\"\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# 1. Tokenization + Lemmatization\n",
        "print(\"TOKENS AND LEMMAS\\n\")\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text:15} Lemma: {token.lemma_}\")\n",
        "\n",
        "# 2. Named Entity Recognition\n",
        "print(\"\\nNAMED ENTITIES\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text:40} Label: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6KpKF1GTMM1",
        "outputId": "3d86d619-d2d5-45e3-b630-c178da5a55c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKENS AND LEMMAS\n",
            "\n",
            "Token: Homi            Lemma: Homi\n",
            "Token: Jehangir        Lemma: Jehangir\n",
            "Token: Bhaba           Lemma: Bhaba\n",
            "Token: was             Lemma: be\n",
            "Token: an              Lemma: an\n",
            "Token: Indian          Lemma: indian\n",
            "Token: nuclear         Lemma: nuclear\n",
            "Token: physicist       Lemma: physicist\n",
            "Token: who             Lemma: who\n",
            "Token: played          Lemma: play\n",
            "Token: a               Lemma: a\n",
            "Token: key             Lemma: key\n",
            "Token: role            Lemma: role\n",
            "Token: in              Lemma: in\n",
            "Token: the             Lemma: the\n",
            "Token: \n",
            "               Lemma: \n",
            "\n",
            "Token: development     Lemma: development\n",
            "Token: of              Lemma: of\n",
            "Token: India           Lemma: India\n",
            "Token: ’s              Lemma: ’s\n",
            "Token: atomic          Lemma: atomic\n",
            "Token: energy          Lemma: energy\n",
            "Token: program         Lemma: program\n",
            "Token: .               Lemma: .\n",
            "Token: He              Lemma: he\n",
            "Token: was             Lemma: be\n",
            "Token: the             Lemma: the\n",
            "Token: founding        Lemma: found\n",
            "Token: director        Lemma: director\n",
            "Token: of              Lemma: of\n",
            "Token: the             Lemma: the\n",
            "Token: Tata            Lemma: Tata\n",
            "Token: \n",
            "               Lemma: \n",
            "\n",
            "Token: Institute       Lemma: Institute\n",
            "Token: of              Lemma: of\n",
            "Token: Fundamental     Lemma: Fundamental\n",
            "Token: Research        Lemma: Research\n",
            "Token: (               Lemma: (\n",
            "Token: TIFR            Lemma: TIFR\n",
            "Token: )               Lemma: )\n",
            "Token: and             Lemma: and\n",
            "Token: was             Lemma: be\n",
            "Token: instrumental    Lemma: instrumental\n",
            "Token: in              Lemma: in\n",
            "Token: establishing    Lemma: establish\n",
            "Token: the             Lemma: the\n",
            "Token: \n",
            "               Lemma: \n",
            "\n",
            "Token: Atomic          Lemma: Atomic\n",
            "Token: Energy          Lemma: Energy\n",
            "Token: Commission      Lemma: Commission\n",
            "Token: of              Lemma: of\n",
            "Token: India           Lemma: India\n",
            "Token: .               Lemma: .\n",
            "\n",
            "NAMED ENTITIES\n",
            "\n",
            "Entity: Homi Jehangir Bhaba                      Label: FAC\n",
            "Entity: Indian                                   Label: NORP\n",
            "Entity: India                                    Label: GPE\n",
            "Entity: the Tata\n",
            "Institute of Fundamental Research Label: ORG\n",
            "Entity: Atomic Energy Commission of India        Label: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working on a chatbot for a mental health platform. Explain how you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford NLP to understand and respond to user input effectively. Detail your architecture, data preprocessing pipeline, and any ethical considerations.**\n",
        "\n",
        "\n",
        "To build a mental-health chatbot, LSTM or GRU networks can be used to understand the context and emotion in user messages. These models are suitable because they capture long-term dependencies in text, such as expressions of sadness or anxiety across multiple sentences. spaCy or Stanford NLP can be used in the initial stage for tokenization, lemmatization, part-of-speech tagging, and named entity recognition to convert raw user input into structured linguistic features.\n",
        "\n",
        "The architecture would start with a preprocessing pipeline where the user text is cleaned, tokenized, and converted into word embeddings. The processed sequence is then passed to an LSTM/GRU layer that performs intent and emotion classification, such as detecting depression, stress, or crisis situations. Based on the predicted intent, a dialogue manager selects an appropriate empathetic response or escalates the conversation to a human counselor when necessary.\n",
        "\n",
        "Ethical considerations are critical in such a system. The chatbot must protect user privacy, avoid storing sensitive personal data, and clearly state that it is not a replacement for professional therapy. It should include safety mechanisms to detect self-harm or suicidal intent and immediately provide helpline information and encourage human support. Regular monitoring and bias-free, compassionate responses are essential to ensure user well-being."
      ],
      "metadata": {
        "id": "bFD8vxPCTpBU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vxsol8dqThVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}